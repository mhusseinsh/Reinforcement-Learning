It took quite long time to figure out how to create the estimator network with tensor flow, since we didn’t have experience with it. 

Then we couldn’t understand how the target network was working: It would have been great to add some explanations for us in the comments.

Then the major problem was the shape of data that we sent to update and predict functions. Luckily we could solve the problems.

Experience part was difficult too due to long training time. We had to run two terminals on several machines during our implementation. 

We spent 25 hours for this assignment… Extra time was a chance for us, but still we couldn’t get the expected convergence whatever we tried. The network and q_learning() seem okay for us, but our graphs aren’t well.

We would like to have a master solution for this assignment, since it was very complicated, and we couldn’t get the correct graphs…

For Q_learning and experiment replay, we had 4 experiments: first we used the given hyperparameters, then as keeping the other three hyperparameters, we changed the other one.

First experiment:
learning rate: 0.005
discount factor: 0.99
epsilon: 0.1

Second experiment:
learning rate: 0.01
discount factor: 0.99
epsilon: 0.1

Third experiment:
learning rate: 0.005
discount factor: 0.5
epsilon: 0.1

Fourth experiment:
learning rate: 0.005
discount factor: 0.99
epsilon: 0.5

In terms of differences between two approaches:

We had variable episode length only with epsilon=0.5 for both approaches, with experiment 4. For the rest of the experiments, we got pretty much the same results. 
In experience replay, episode rewards decreased first, then increased again. In Q_learning, the rewards were fluctuating constantly. 

